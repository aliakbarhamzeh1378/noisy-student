{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport os\nimport shutil\n\nfrom keras import Input\nfrom keras.applications.efficientnet import EfficientNetB0\nfrom keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import GlobalAveragePooling2D, BatchNormalization, Dropout, Dense\nfrom keras.models import load_model, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom PIL import ImageFile\nfrom PIL import Image\n\nimport sys\nsys.modules['Image'] = Image \nImageFile.LOAD_TRUNCATED_IMAGES = True\n",
      "metadata": {},
      "execution_count": 1,
      "outputs": [],
      "id": "067fbdd6-7b4a-416d-b85b-209f611354b1"
    },
    {
      "cell_type": "code",
      "source": "\nclass NoisyStudent:\n    def __init__(self, labelled_dataset: str, unlabelled_dataset: str, labels: list, number_of_train: int = 50,\n                 teacher_model_path: str = \"model.h5\",\n                 teacher_model_size: tuple = (224, 224),\n                 student_model_path: str = None,\n                 confidence: float = 0.90, batch_size: int = 70, epochs: int = 4, learning_rate: float = 0.001,\n                 loss_function: str = \"categorical_crossentropy\"):\n        self.confidence = confidence\n        self.labelled_dataset = labelled_dataset\n        self.unlabelled_dataset = unlabelled_dataset\n        self.labels = labels\n        self.teacher_model_path = teacher_model_path\n        self.teacher_model = None\n        self.teacher_model_size = teacher_model_size\n        self.student_model_path = student_model_path\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.loss_function = loss_function\n        self.num_classes = len(self.labels)\n        self.number_of_train = number_of_train\n        self.__check_require_data()\n        pass\n\n    def __check_require_data(self):\n        if not os.path.exists(self.labelled_dataset):\n            print( \"Labelled dataset not found\")\n            exit(0)\n        if not os.path.exists(self.unlabelled_dataset):\n            print( \"Unlabelled dataset not found\")\n            exit(0)\n\n        if len(self.labels) == 0:\n            print( \"not labels set\")\n            exit(0)\n        for obj_class in self.labels:\n            if obj_class not in os.listdir(self.labelled_dataset):\n                print( \"label not exist in labelled dataset\")\n                exit(0)\n        if self.teacher_model_path != \"\":\n            if not os.path.exists(self.teacher_model_path):\n                print( \"teacher model not found\")\n                exit(0)\n            self.teacher_model = self.__load_model(self.teacher_model_path)\n        else:\n            self.teacher_model_path = \"model.h5\"\n        return\n\n\n    def __load_model(self, path: str) -> load_model:\n        try:\n            model = load_model(path)\n        except Exception as e:\n            raise \"teacher model load error : \" + str(e)\n        return model\n\n    def __preprocess_dataset(self):\n        dataset_generator = ImageDataGenerator(\n            rotation_range=10,  # rotation\n            width_shift_range=0.2,  # horizontal shift\n            height_shift_range=0.2,  # vertical shift\n            zoom_range=0.2,  # zoom\n            horizontal_flip=True,  # horizontal flip\n            brightness_range=[0.2, 1.2],\n            validation_split=0.3\n        )\n\n        __train_generator = dataset_generator.flow_from_directory(\n            directory=self.labelled_dataset,\n            target_size=self.teacher_model_size,\n            class_mode='categorical',\n            color_mode=\"rgb\",\n            batch_size=self.batch_size,\n            shuffle=True,\n            seed=2020,  # to make the result reproducible\n            subset='training'\n        )\n        __validate_generator = dataset_generator.flow_from_directory(\n            directory=self.labelled_dataset,\n            target_size=self.teacher_model_size,\n            class_mode='categorical',\n            color_mode=\"rgb\",\n            batch_size=self.batch_size,\n            shuffle=True,\n            seed=2020,  # to make the result reproducible\n            subset='validation'\n        )\n        return __train_generator, __validate_generator\n\n    def __monitoring_initial(self):\n        earlystop = EarlyStopping(patience=10)\n        tensorboard = TensorBoard(log_dir=\"~/logs\", histogram_freq=1, update_freq='batch', profile_batch=True,\n                                  write_graph=True, write_images=True, write_steps_per_second=True)\n        checkpoint = ModelCheckpoint(filepath=\"checkpoints/\", save_weights_only=False, monitor='val_accuracy',\n                                     mode='max',\n                                     save_best_only=True)\n        learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.5,\n                                                    min_lr=0.0001)\n        return [earlystop, learning_rate_reduction, checkpoint, tensorboard]\n\n    def __model_builder(self):\n        inputs = Input(shape=(self.teacher_model_size[0], self.teacher_model_size[1], 3))\n        model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n\n        # Freeze the pretrained weights\n        model.trainable = False\n\n        # Rebuild top\n        x = GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n        x = BatchNormalization()(x)\n\n        top_dropout_rate = 0.2\n        x = Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n        outputs = Dense(self.num_classes, activation=\"softmax\", name=\"pred\")(x)\n\n        # Compile\n        model = Model(inputs, outputs, name=\"EfficientNet\")\n        optimizer = Adam(lr=self.learning_rate)\n        model.compile(\n            optimizer=optimizer, loss=self.loss_function, metrics=[\"accuracy\", \"MeanSquaredError\", \"AUC\"]\n        )\n        return model\n\n    def train(self):\n        train_generator, validate_generator = self.__preprocess_dataset()\n        model = self.__model_builder()\n        callbacks = self.__monitoring_initial()\n        if self.teacher_model_path != None:\n            if os.path.exists(self.teacher_model_path):\n                model.load_weights(self.teacher_model_path)\n        model.fit(\n            train_generator,\n            epochs=self.epochs,\n            validation_data=validate_generator,\n            validation_steps=validate_generator.samples // self.batch_size,\n            steps_per_epoch=train_generator.samples // self.batch_size,\n            callbacks=callbacks\n        )\n        model.save(self.teacher_model_path)\n\n    def main(self):\n        print(\"start task\")\n        print(\"number of train \",self.number_of_train)\n        for i in range(self.number_of_train):\n            print(\"===================== train ================\")\n            self.train()\n            print(\"===================== predict ================\")\n            self.predict_unlabelled_data()\n            \n\n    def predict_unlabelled_data(self):\n        print(\"start predict\")\n        image_width, image_height = self.teacher_model_size\n        files = os.listdir(self.unlabelled_dataset)\n        predict_batch = int(len(files) / self.batch_size)\n        self.teacher_model = self.__load_model(self.teacher_model_path)\n        try:\n            for batch in range(0, predict_batch):\n                predict_files = files[batch * self.batch_size:(batch + 1) * self.batch_size]\n                images = []\n                images_path = []\n                for image_name in predict_files:\n                    img = os.path.join(self.unlabelled_dataset, image_name)\n                    images_path.append(img)\n                    img = image.load_img(img, target_size=(image_width, image_height))\n                    img = image.img_to_array(img)\n                    img = np.expand_dims(img, axis=0)\n                    images.append(img)\n                images_data = np.vstack(images)\n                classes = self.teacher_model.predict(images_data, batch_size=self.batch_size)\n                for file_number, predict in enumerate(classes):\n                  \n                    if predict[np.argmax(predict)] > self.confidence:\n                        print(\n                            f\"{images_path[file_number]} is {self.labels[np.argmax(predict)]} score {predict[np.argmax(predict)]}\")\n                        image_path = images_path[file_number]\n                        image_name = image_path.split('/')[-1]\n                        shutil.move(image_path,\n                                    self.labelled_dataset + '/' + self.labels[np.argmax(predict)] + '/' + image_name)\n        except Exception as e:\n            print(image_name)\n            print(e)\n\nns = NoisyStudent(labelled_dataset=\"labled_data/train\", unlabelled_dataset=\"unlabled_data/\", labels=[\"class1\", \"class2\"])\n",
      "metadata": {},
      "execution_count": 8,
      "outputs": [],
      "id": "43a257ec-0786-4a07-af26-ec0d47ba9086"
    },
    {
      "cell_type": "code",
      "source": "ns.main()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "01196bb1-7841-484a-b46b-bdfbe1e58842"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "d42db4fb-463a-4524-a874-71319f4aac8d"
    }
  ]
}